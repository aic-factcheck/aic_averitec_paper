%!TEX ROOT=../emnlp2023.tex

\section{Classification Step}
In this section, we describe the classification component of our pipeline. First, we introduce chronologically the different approaches, the ensembles we tried, and their pros and cons. Then, we present evaluation results across different methods and various metrics. The goal of this step is to take the input claim and questions with answers provided by previous stages \todo{ref} and classify the claim into one of the four classes: \textit{Supported}, \textit{Refuted}, \textit{Not Enough Evidence}, or \textit{Conflicting Evidence/Cherrypicking} as defined by~\citealp{averitec2024}.

\subsection{Approaches}
\todo{Describe what was done +  why}

In the earliest stages of experimenting, we utilized the classifier from baseline provided by authors\footnote{https://huggingface.co/chenxwh/AVeriTeC}~\cite{averitec2024}. This classifier is based on BERT~\cite{devlin-etal-2019-bert} model and was further fine-tuned on the AVeriTeC dataset~\cite{averitec2024}. It takes one claim and one question with its answer as input. The output of this encoder model (class logits) is then further processed by several if-clauses to determine the final label. To the best of our knowledge, the complete classifier outputs \textit{Not Enough Evidence} every time when the model predicts the \textit{Not Enough Evidence} or some 4th label (here we are not sure how the model was exactly fine-tuned) for any of the (up to 10) question-anwer pairs. It predicts \textit{Supported} if there is no \textit{Not Enough Evidence} prediction and at least one \textit{Supported} prediction and no \textit{Refuted} prediction. Then it predicts \textit{Refuted} if there are no \textit{Not Enough Evidence} or \textit{Supported} predictions and at least one \textit{Refuted} prediction. Otherwise, it predicts \textit{Conflicting Evidence/Cherrypicking}. Despite considerable effort, we were unable to understand why and how was the model trained on four labels and also what is the exact background of this, at least from our point of view, ad-hoc post-processing logic.

We argue that the post-processing logic should be changed so that \textit{Not Enough Evidence} is predicted only if there is no other label to predicted. We think that for example if among the predictions for question-answer pairs there is one \textit{Not Enough Evidence} prediction and the rest are \textit{Supported} predictions, the final prediction should be \textit{Supported} and not \textit{Not Enough Evidence} as in the original logic. We did implemented this change (see listing~\ref{lst:post-processing}) and surprisingly it did not improve the results as the number of \textit{Not Enough Evidence} class has fallen significantly \todo{numbers}. Despite that, we still think that this change is more logical and should be used in the future.

\begin{figure}
    \begin{lstlisting}[language=Python]
if has_true and has_false:
  answer = 3
elif has_true and not has_false:
  answer = 0
elif not has_true and has_false:
  answer = 1
else:
 answer = 2 #otherwise NEI
        \end{lstlisting}
    \caption{Our proposed post-processing logic} 
    \label{lst:post-processing}       
\end{figure}

Moreover, we think that the model should be trained only on three labels because only three are used. We tried own fine-tuning of a newer encoder model DeBERTaV3~\cite{he2023debertav3improvingdebertausing} on only three labels and the results were better \todo{numbers}.

All of the results of our preliminary experimenting were however not satysfing and we decided to try more radically different approaches that are described in the following sections.

\subsubsection*{Concatenation and Four Classes Classificator}
\todo{better name}
\todo{describe, models, Concatenation, sep-concatenation, random order, four classes!, Mistral, DeBERTaV3, metion Transformers-HF}
Our first approach is to finetune a text classification model on all four classes directly. However, this means that as evidence, we must feed the model with all question-answer pairs at once to classify \textit{Conflicting Evidence/Cherrypicking} correctly. For that, we tried several options. First, we concatenated all of them using single blank space in the order they are provided in the dataset. This should work because we expect that annotators created the question-answer pairs usually in a specific order. The second option we tried is concatenation using a separator \texttt{[SEP]} token again in the original order. This option utilizes the model's knowledge about separator tokens and allows it to know exact question-answer borders. The last option we tried is to randomly select more orders of the question-answer pairs (all combinations computation is not feasible - up to $10!$ of combinations).

As models, we chose DeBERTaV3~\cite{he2023debertav3improvingdebertausing} in two variants: the original large one\footnote{https://huggingface.co/microsoft/deberta-v3-large} and one pre-finetuned on NLI tasks\footnote{https://huggingface.co/cross-encoder/nli-deberta-v3-large}, and also Mistral-7B-v0.3 model\footnote{https://huggingface.co/mistralai/Mistral-7B-v0.3} with a classification head (MistralForSequenceClassification) provided by the Huggingface Transformers library~\cite{wolf-etal-2020-transformers} that utilizes the last token. In the preliminary testing phase, the original DeBERTaV3 Large performed the best and was used in all other experimental settings.

From the approaches described above, we achieved the best results with the original order achieving 0.71 macro $F_1$ score. The separator model achieved a comparable 0.70 macro $F_1$ score, and the random order model performed worse with a 0.67 macro $F_1$ score. We provide our best DeBERTaV3 finetuned model publicly in a Huggingface repository\footnote{\todo{https://huggingface.co/ctu-aic/deberta-v3-large-AVeriTeC-nli}}.

\subsubsection*{LLM Classifiers}
\todo{describe - likert motivation, GPT4o, Claude 3?}
\todo{try "opensource" LLMs? - maybe own section in an Appendix?}

\subsubsection*{Ensembling}
\todo{describe ensembling - average, weighted average, stacking using logreg}

\subsubsection*{Conflicting Evidence/Cherrypicking Detection}
\todo{Binary deberta with custom loss function - weighted crossentropy loss}
\todo{Bryce's idea - tf-idf + randomforests?, future works-new paper (not exactly what we are doing now, but interesting)~\cite{jaradat2024contextawaredetectioncherrypickingnews}}


\subsection{Classification Evaluation}
\todo{table with comparative results + comments}
