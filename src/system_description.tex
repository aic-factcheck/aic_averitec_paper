%!TEX ROOT=../emnlp2023.tex

\section{System Description}
\label{sec:system}
Our pipeline for fact-checking claims using evidence retrieved from the web consists of two modules -- a retriever, which picks the most relevant sources among the available knowledge store\footnote{Due to the pre-retrieval step that was used to generate knowledge stores, our \say{retriever} module could more conventionally be referred to as a \say{reranker}, which we refrain from, to avoid confusion with reranking steps it uses as a subroutine.} and an evidence generator which generates evidence for the claim using these sources, as well as its veracity label. 

In this chapter, we also describe a third, optional module we call veracity classifier, which takes the outputs of the evidence generator and combines the label suggested in the generation step with predictions of a NLI model fine-tuned of our task using ensembling.
The absence of a dedicated veracity classifier has not been shown to decrease the performance of our pipeline throughout experimentation (as shown, e.g., in Table~\ref{tab:nli}) so we suggest its omission and proceed to participate in the Averitec shared task without it, proposing a clean and simple RAG pipeline (Figure~\ref{fig:pipeline}) for the fact-checking task.

\subsection{Retrieval Module}
To ease comparison with the baseline and other systems designed for the task, our system does not use direct internet/search-engine access for its retrieval, but a \textit{knowledge store} provided separately alongside each claim.

\todo{vypichnout že místo sentencí co používají v baseline my preferujeme chunky, které jsou mnohem lepší semanticka jednotka -- podpořit zdroji z původního feveru}

\subsubsection{Knowledge Stores}
Each claim's knowledge store contains pre-scraped search results for various queries that can be derived from the claim using human annotation or generative models.
The knowledge stores used with ours as well as the baseline system can be downloaded from the Averitec dataset page\footnote{\url{https://fever.ai/dataset/averitec.html}}, containing about 1000 pre-scraped \textit{documents}\footnote{\label{devsetnote} The numbers are orientational and were established using the dev knowledge store}, each consisting of $28$ sentences at median\footnoteref{devsetnote}, albeit varying wildly between documents.

To use our system in the wild, this knowledge store can be emulated using a search API such as SerpApi, or even a large document collection such as Common Crawl pruned down to similar orders of magnitude using a cheap retrieval method and the claim as a search query.

Our retrieval module then focuses on picking a set of $k$ ($k=10$ in the examples below) most appropriate document chunks to fact-check the provided claim within this knowledge store.

\subsubsection{Chunking}
Our initial experiments with the whole AVerITeC documents for the Document Retrieval step have revealed a significant weakness -- while the median document length (about 2000 characters) often fits the input size of the embedding model we use for semantical search (see Section~\ref{sec:knn}), outliers are common, with \textit{hundreds of thousands} characters, exceeding the 512 input tokens with little to no coverage of their content.

Upon further examination, these have more often than not, been PDF documents of legislature, documentation and communication transcription -- highly relevant sources real fact-checker would scroll through to find the relevant part to refer. 

This workflow resembles the traditional chunk retrieval~\todo{cite} typical for RAG which we use in the following fashion: each document is partitioned into sets of sentences of combined length of $n$ characters at most.
To take advantage of the full input size of the vector embedding model we use for semantical search, we set our bound $n=512*4=2048$, 512 being the input dimension of common embedding models, 4 being a typical number of characters per token for US English in modern tokenizers. \todo{zdroj?}

Each chunk is assigned metadata -- the source URL, as well as the full text of the next and previous chunk within the same document.
This way, chunks can be presented to the LLM along with their original context in the generation module, where the length constraint is much less of an issue than in vector embedding.

\subsubsection{Pruning the chunks}
While the chunking of long articles prevents information from larger documents from being lost, it makes the search domain too large to embed.
As each of the thousands of claims has its own retrieval domain of possibly tens of thousands of chunks, we seek to omit the chunks having little to no common tokens with our claim using an efficient BM25~\cite{bm25} search for the nearest $k$ chunks, setting the $k$ to 6000 for dev and 2000 for test claims. 
This yields a reasonably-sized document store for embedding each chunk into a vector, taking an average of 40s to compute and store using the method described in Section~\ref{sec:knn} for each test-claim using our Tesla V100 GPU.

This allows a quick and agile production of vectorstores for further querying and experimentation, motivated by the Averitec test data being published just several days before the announced submission deadline.
The pruning also keeps the resource intensity moderate for real-world applications -- if time is not of the essence, the step can be omitted.

\subsubsection{Angle-optimized embedding search}
\label{sec:knn}
\todo{možná posunout před chunking}
Mixedbread~\cite{li-li-2024-aoe,emb2024mxbai}, Faiss~\cite{douze2024faiss,johnson2019billion}

\subsubsection{Diversifying sources: MMR}
While the original~\cite{averitec2024} baseline retrieved articles based on different queries to promote variety among search results, our approach omits the division of a claim to a set of different queries.
We aim to use an embedding-driven similarity search in the neighbourhood of the whole original claim not to leave any piece of information behind or introduce noise in yet another generative task along the pipeline.

Our solution is, however, prone to redundancy among search results, which we address using a reranking by the results' Maximal Marginal Relevance (MMR)~\cite{carbonell-mmr}, a metric popular for the RAG task computed as (for $D_i\in P$)
$$\lambda \cdot \mathrm{Sim}(D_i, Q) - (1-\lambda) \cdot \max_{D_j \in S} \mathrm{Sim}(D_i, D_j)$$
$Sim$ denoting the cosine-similarity among document embeddings, $Q$ being the search query, and $P$ denoting the pre-fetched documents by simply optimizing the $Sim$ to $Q$, $S$ being the search result.

In our system, we set $\lambda=0.75$ to favour relevancy rather than diversity, $|S|=10$ and $|P| = 40$, obtaining a set of diverse sources relevant to each claim at a fraction of cost and complexity of a query-generation driven retrieval, such as that used in~\cite{averitec2024}.

\subsection{Evidence Generation}
Rather than sampling evidence from retrieved text or QG+QA pipeline ran on the retrieved sentences, we argue that the modern LLMs offer the use of wider context, such as whole news articles.

\subsubsection{\texttt{JSON} Generation}
The current LLMs are trained very well for this, allows for very simple integration of LLM into pipeline

\subsubsection{Source Referring}
We assign a 1-based index to each of the sourced chunks and prompt the LLM to refer it as the source ID with each evidence it generates.
This has been shown quite reliable in \todo{literature}.

\subsubsection{Chain-of-thoughts Prompting}
While JSON dictionary should be order-invariant, we can actually exploit the order of outputs to make LLMS like GPT-4o output better results.

\subsubsection{Few-shot learning}
To further boost our system's evidence-generation capabilities, 


