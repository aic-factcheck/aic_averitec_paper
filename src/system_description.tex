%!TEX ROOT=../emnlp2023.tex

\section{System Description}
\label{sec:system}
Our best-performing pipeline for fact-checking claims using evidence retrieved from the web consists of two modules -- a \textit{retriever}, which picks the most relevant sources among the available knowledge store\footnote{Due to the pre-retrieval step that was used to generate knowledge stores, our \say{retriever} module could more conventionally be referred to as a \say{reranker}, which we refrain from, to avoid confusion with reranking steps it uses as a subroutine.} and an \textit{evidence \& label generator} which generates evidence for the claim using these sources, as well as its veracity label. 
It is a variant of Retrieval-augmented Generation (RAG) scheme~\cite{rag}, making it easy to re-implement using established frameworks such as Langchain, Haystack, or our attached Python codebase for future research or to use as a baseline.

This chapter describes our pipeline and the decisions taken at each module, achieving a simple yet efficient RAG scheme that improves dramatically across the board over the baseline system from~\cite{averitec2024}, and scores third in the \averitec{}  leaderboard as of August 2024.

\subsection{Retrieval Module}
To ease comparison with the baseline and other systems designed for the task, our system does not use direct internet/search-engine access for its retrieval, but an \averitec{} \textit{knowledge store} provided separately alongside each claim.

To use our pipeline in the wild, our retrieval module is decoupled from the rest of the pipeline and can be swapped out in favour of an internet search API such as SerpApi as a whole, or it can be used with knowledge store emulated using search API or a cheap on-premise search over large crawled corpora such as CommonCrawl.

\subsubsection{Knowledge Stores}
Each claim's knowledge store contains pre-scraped results for various search queries that can be derived from the claim using human annotation or generative models.
The knowledge stores used with ours as well as the baseline system can be downloaded from the \averitec{}  dataset page\footnote{\url{https://fever.ai/dataset/averitec.html}}, containing about 1000 pre-scraped \textit{documents}\footnote{\label{devsetnote} The numbers are orientational and were computed on knowledge stores provided for the \averitec{}  dev set.}, each consisting of $28$ sentences at median\footnoteref{devsetnote}, albeit varying wildly between documents.
The methods used for generating the knowledge stores are explained in more detail in~\cite{averitec2024}.
%To use our system in the wild, this knowledge store can be emulated using a search API such as SerpApi, or even a large document collection such as Common Crawl pruned down to similar orders of magnitude using a cheap retrieval method and the claim as a search query.

Our retrieval module then focuses on picking a set of $k$ ($k=10$ in the examples below, as well as in our submitted system) most appropriate document chunks to fact-check the provided claim within this knowledge store.

\subsubsection{Angle-optimized embedding search}
\label{sec:knn}
Despite each article in any knowledge store only needing to be compared \textit{once} with its \textit{one specific} claim, which should be the use-case for CrossEncoder reranking~\cite{dejean2024thoroughcomparisoncrossencodersllms}, our empirical preliminary experiments made us favour a \textit{cosine-similarity} search based on vector embeddings instead.
It takes less time to embed the whole knowledge store into vectors than to match each document against a claim using crossencoder, and the produced embeddings can be re-used across experiments.

For our proof of concept, we explore the MTEB~\cite{muennighoff-etal-2023-mteb} benchmark leaderboard, looking for a reasonably-sized open-source embedding model, ultimately picking Mixedbread's mxbai-large-v1~\cite{li-li-2024-aoe,emb2024mxbai} optimized for the cosine objective fitting our inteded use.

To reduce querying time at a reasonable exactness tradeoff, we use Faiss~\cite{douze2024faiss,johnson2019billion} index to store our vectors, allowing us to only precompute semantical representation once, making the retriever respond rapidly in empirical experiments, allowing a very agile prototyping of novel methods to be used.

\subsubsection{Chunking with added context}
Our initial experiments with the whole \averitec{}  documents for the Document Retrieval step have revealed a significant weakness -- while most documents fit within the input size of the embedding model, outliers are common, often with \textit{hundreds of thousands} characters, exceeding the 512 input tokens with little to no coverage of their content.

Upon further examination, these are typically PDF documents of legislature, documentation and communication transcription -- highly relevant sources real fact-checker would scroll through to find the relevant part to refer. 

This workflow resembles the chunk retrieval as used in~\cite{rag}, popularized for use with RAG.
We partition each document into sets of sentences of combined length of $n$ characters at most.
To take advantage of the full input size of the vector embedding model we use for semantical search, we set our bound $n=512*4=2048$, 512 being the input dimension of common embedding models, 4 often being used as a rule-of-thumb number of characters per token for US English in modern tokenizers~\cite{tokens}.

Importantly, each chunk is  assigned metadata -- the source URL, as well as the full text of the next and previous chunk within the same document.
This way, chunks can be presented to the LLM along with their original context in the generation module, where the length constraint is much less of an issue than in vector embedding.
As shown in~\cite{drchal2023pipelinedatasetgenerationautomated}, fact-checking models benefit from being exposed to larger pieces of text such as paragraphs or entire documents rather than out-of-context sentences.
Splitting our data into the maximum chunks that fit our retrieval model and providing them with additional context may help down the line, preventing the RAG sources from being semantically incomplete.

\subsubsection{Pruning the chunks}
While the chunking of long articles prevents their information from getting lost to retriever, it makes its search domain too large to embed on demand.
As each of the thousands of claims has its own knowledge store, each of possibly tens of thousands of chunks, we seek to omit the chunks having little to no common tokens with our claim using an efficient BM25~\cite{bm25} search for the nearest $\Omega$ chunks, setting the $\Omega$ to 6000 for dev and 2000 for test claims. 
This yields a reasonably-sized document store for embedding each chunk into a vector, taking an average of 40s to compute and store using the method described in Section~\ref{sec:knn} for each dev-claim using our Tesla V100 GPU.

This allows a quick and agile production of vectorstores for further querying and experimentation, motivated by the \averitec{}  test data being published just several days before the announced submission deadline.
The pruning also keeps the resource intensity moderate for real-world applications -- if time is not of the essence, the step can be omitted.

\subsubsection{Diversifying sources: MMR}
While the original~\cite{averitec2024} baseline retrieved articles based on various different search queries to promote variety among search results, our approach omits this generation of \say{subqueries}.
We aim to use an embedding-driven similarity search in the neighbourhood of the \textit{whole} original claim not to leave any piece of information behind or introduce other noise during the subquery generation.

Our solution is, however, prone to redundancy among search results, which we address using a reranking by the results' Maximal Marginal Relevance (MMR)~\cite{carbonell-mmr}, a metric popular for the RAG task, which maximizes the search results' score computed as (for $D_i\in P$)
$$\lambda \cdot \mathrm{Sim}(D_i, Q) - (1-\lambda) \cdot \max_{D_j \in S} \mathrm{Sim}(D_i, D_j)$$
$Sim$ denoting the cosine-similarity between embeddings, $Q$ being the search query, and $P$ the pre-fetched set of documents (by a search which simply maximizes their $Sim$ to $Q$), forming $S$ as the final search result, by adding each $D_i$ as MMR-argmax one by one, until reaching its desired size.

In our system, we set $\lambda=0.75$ to favour relevancy rather than diversity, $|S|=10$ and $|P| = 40$, obtaining a set of diverse sources relevant to each claim at a fraction of cost and complexity of a query-generation driven retrieval, such as that used in~\cite{averitec2024}.

\subsection{Evidence \& Label Generation}
\label{sec:generation}
Rather than sampling evidence from retrieved text or QG+QA pipeline ran on the retrieved sentences, we argue that the modern LLMs offer the use of wider context, such as whole news articles.

\subsubsection{JSON Generation}
The current LLMs are trained very well for this~\cite{json}, allows for very simple integration of LLM into pipeline

\subsubsection{Chain-of-thought Prompting}
While JSON dictionary should be order-invariant, we can actually exploit the order of outputs to make LLMS like GPT-4o output better results.~\cite{cot}

\subsubsection{Source Referring}
We assign a 1-based index to each of the sourced chunks and prompt the LLM to refer it as the source ID with each evidence it generates.
This has been shown quite reliable in \cite{menick2022teachinglanguagemodelssupport}.

\subsubsection{Few-shot learning}
Few-shot learning by giving examples within prompt has been shown to boost performance in generative task already since GPT-3~\cite{fewshot}.
To further boost our system's evidence-generation capabilities, 

\subsubsection{Likert-scale Label Confidence}
\label{likert}
Despite modern LLMs being well capable of predicting the label in a \say{pick one} fashion, research applications such as ours may prefer them to output a probability distribution over all labels for two reasons.

Firstly, it measures the confidence in each label, pinpointing the edge-cases, secondly, it allows ensembling the LLM classification with any other model, such as Encoders with classification head finetuned on the task of Natural Language Inference (NLI) (see section~\ref{subsubsec:ensembling}).

As the LLMs and other token prediction schemes struggle with the prediction of continuous numbers which are notoriously hard to tokenize appropriately~\cite{golkar2023xvalcontinuousnumberencoding}, we come up with a simple alternative: using zero-shot learning, model is prompted to print each of the 4 possible labels, along with their Likert-scale rating: 1 for \say{strongly disagree}, 2 for \say{disagree}, 3 for \say{neutral}, 4 for \say{agree} and 5 for \say{strongly agree}~\cite{likert1932technique}.

On top of the ease of tokenization, Likert scale's popularity in psychology and other fields such as software testing~\cite{likertstudy} adds another benefit -- both the scale itself and its appropriate usage was likely demonstrated many times to LLMs during their unsupervised training phase.

To convert the ratings such as \texttt{\{\say{Supported}:2, \say{Refuted}:5, \say{Cherrypicking}:4, \say{NEE}:2\}} to a kind of probability distribution, we simply use softmax~\cite{NIPS1989_0336dcba}.
While the label probabilities are only emulated (and may only take a limited, discrete set of values) and the system may produce ties, it gets the job done until further research is carried out.

\subsubsection{Choosing LLM}
In our experiments, we have tested the full set of techniques introduced in this section, computing the text completion requests with:
\begin{enumerate}
    \item GPT-4o (version \texttt{2024-05-13})
    \item Claude-3.5-Sonnet (\texttt{2024-06-20}), using the Google's Vertex API
    \item LLaMA 3.1 70B, in the final experimets to see if the pipeline can be re-produced using open-source models
\end{enumerate} 

Their comparison can be seen in tables~\ref{tab:nli} and~\todo{tab}; for our submission in the \averitec{}  shared task, GPT-4o was used.