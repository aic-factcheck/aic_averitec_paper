%!TEX ROOT=../emnlp2023.tex

\section{System Description}
\label{sec:system}
Our best-performing pipeline for fact-checking claims using evidence retrieved from the web consists of two modules -- a \textit{retriever}, which picks the most relevant sources among the available knowledge store\footnote{Due to the pre-retrieval step that was used to generate knowledge stores, our \say{retriever} module could more conventionally be referred to as a \say{reranker}, which we refrain from, to avoid confusion with reranking steps it uses as a subroutine.} and an \textit{evidence \& label generator} which generates evidence for the claim using these sources, as well as its veracity label. 
It is a variant of Retrieval-augmented Generation (RAG) scheme~\cite{rag}, making it easy to re-implement using established frameworks such as Langchain, Haystack, or our attached Python codebase for future research or to use as a baseline.

This chapter describes our pipeline and the decisions taken at each module, achieving a simple yet efficient RAG scheme that improves dramatically across the board over the baseline system from~\cite{averitec2024}, and scores third in the \averitec{}  leaderboard as of August 2024.

\subsection{Retrieval Module}
To ease comparison with the baseline and other systems designed for the task, our system does not use direct internet/search-engine access for its retrieval, but a \textit{knowledge store} provided separately alongside each claim.

\todo{vypichnout že místo sentencí co používají v baseline my preferujeme chunky, které jsou mnohem lepší semanticka jednotka -- podpořit zdroji z původního feveru}

\subsubsection{Knowledge Stores}
Each claim's knowledge store contains pre-scraped search results for various queries that can be derived from the claim using human annotation or generative models.
The knowledge stores used with ours as well as the baseline system can be downloaded from the \averitec{}  dataset page\footnote{\url{https://fever.ai/dataset/averitec.html}}, containing about 1000 pre-scraped \textit{documents}\footnote{\label{devsetnote} The numbers are orientational and were computed on knowledge stores provided for the \averitec{}  dev set.}, each consisting of $28$ sentences at median\footnoteref{devsetnote}, albeit varying wildly between documents.

To use our system in the wild, this knowledge store can be emulated using a search API such as SerpApi, or even a large document collection such as Common Crawl pruned down to similar orders of magnitude using a cheap retrieval method and the claim as a search query.

Our retrieval module then focuses on picking a set of $k$ ($k=10$ in the examples below) most appropriate document chunks to fact-check the provided claim within this knowledge store.

\subsubsection{Angle-optimized embedding search}
\label{sec:knn}
\todo{možná posunout před chunking}
Mixedbread~\cite{li-li-2024-aoe,emb2024mxbai}, Faiss~\cite{douze2024faiss,johnson2019billion}

\subsubsection{Chunking with added context}
Our initial experiments with the whole \averitec{}  documents for the Document Retrieval step have revealed a significant weakness -- while most documents fit the input size of the embedding model we use for semantical search (see Section~\ref{sec:knn}), outliers are common, with \textit{hundreds of thousands} characters, exceeding the 512 input tokens with little to no coverage of their content.

Upon further examination, these have more often than not, been PDF documents of legislature, documentation and communication transcription -- highly relevant sources real fact-checker would scroll through to find the relevant part to refer. 

This workflow resembles the chunk retrieval as used in~\cite{rag}, popularized for use with RAG.
We partition each document into sets of sentences of combined length of $n$ characters at most.
To take advantage of the full input size of the vector embedding model we use for semantical search, we set our bound $n=512*4=2048$, 512 being the input dimension of common embedding models, 4 often being used as a rule-of-thumb number of characters per token for US English in modern tokenizers~\cite{tokens}.

Importantly, each chunk is  assigned metadata -- the source URL, as well as the full text of the next and previous chunk within the same document.
This way, chunks can be presented to the LLM along with their original context in the generation module, where the length constraint is much less of an issue than in vector embedding.

\subsubsection{Pruning the chunks}
While the chunking of long articles prevents information from larger documents from being lost, it makes the search domain too large to embed.
As each of the thousands of claims has its own retrieval domain of possibly tens of thousands of chunks, we seek to omit the chunks having little to no common tokens with our claim using an efficient BM25~\cite{bm25} search for the nearest $k$ chunks, setting the $k$ to 6000 for dev and 2000 for test claims. 
This yields a reasonably-sized document store for embedding each chunk into a vector, taking an average of 40s to compute and store using the method described in Section~\ref{sec:knn} for each test-claim using our Tesla V100 GPU.

This allows a quick and agile production of vectorstores for further querying and experimentation, motivated by the \averitec{}  test data being published just several days before the announced submission deadline.
The pruning also keeps the resource intensity moderate for real-world applications -- if time is not of the essence, the step can be omitted.

\subsubsection{Diversifying sources: MMR}
While the original~\cite{averitec2024} baseline retrieved articles based on different queries to promote variety among search results, our approach omits the division of a claim to a set of different queries.
We aim to use an embedding-driven similarity search in the neighbourhood of the whole original claim not to leave any piece of information behind or introduce noise in yet another generative task along the pipeline.

Our solution is, however, prone to redundancy among search results, which we address using a reranking by the results' Maximal Marginal Relevance (MMR)~\cite{carbonell-mmr}, a metric popular for the RAG task, which maximites the search results' score computed as (for $D_i\in P$)
$$\lambda \cdot \mathrm{Sim}(D_i, Q) - (1-\lambda) \cdot \max_{D_j \in S} \mathrm{Sim}(D_i, D_j)$$
$Sim$ denoting the cosine-similarity between embeddings, $Q$ being the search query, and $P$ the pre-fetched set of documents (by a search which simply maximizes their $Sim$ to $Q$), forming $S$ as the final search result, by adding each $D_i$ as MMR-argmax one by one, until reaching its desired size.

In our system, we set $\lambda=0.75$ to favour relevancy rather than diversity, $|S|=10$ and $|P| = 40$, obtaining a set of diverse sources relevant to each claim at a fraction of cost and complexity of a query-generation driven retrieval, such as that used in~\cite{averitec2024}.

\subsection{Evidence \& Label Generation}
\label{sec:generation}
Rather than sampling evidence from retrieved text or QG+QA pipeline ran on the retrieved sentences, we argue that the modern LLMs offer the use of wider context, such as whole news articles.

\subsubsection{JSON Generation}
The current LLMs are trained very well for this, allows for very simple integration of LLM into pipeline

\subsubsection{Source Referring}
We assign a 1-based index to each of the sourced chunks and prompt the LLM to refer it as the source ID with each evidence it generates.
This has been shown quite reliable in \todo{literature}.

\subsubsection{Chain-of-thought Prompting}
While JSON dictionary should be order-invariant, we can actually exploit the order of outputs to make LLMS like GPT-4o output better results.~\cite{cot}

\subsubsection{Few-shot learning}
Few-shot learning by giving examples within prompt has been shown to boost performance in generative task already since GPT-3~\cite{fewshot}.
To further boost our system's evidence-generation capabilities, 

\subsubsection{Likert-scale Label Confidence}
\label{likert}
Despite modern LLMs being well capable of predicting the label in a \say{pick one} fashion, research applications such as ours may prefer them to output a probability distribution over all labels for two reasons.

Firstly, it measures the confidence in each label, pinpointing the edge-cases, secondly, it allows ensembling the LLM classification with any other model, such as Encoders with classification head finetuned on the task of Natural Language Inference (NLI) (see section~\ref{subsubsec:ensembling}).

As the LLMs and other token prediction schemes struggle with the prediction of continuous numbers which are notoriously hard to tokenize appropriately~\cite{golkar2023xvalcontinuousnumberencoding}, we come up with a simple alternative: using zero-shot learning, model is prompted to print each of the 4 possible labels, along with their Likert-scale rating: 1 for \say{strongly disagree}, 2 for \say{disagree}, 3 for \say{neutral}, 4 for \say{agree} and 5 for \say{strongly agree}~\cite{likert1932technique}.

On top of the ease of tokenization, Likert scale's popularity in psychology and other fields such as software testing~\cite{likertstudy} adds another benefit -- both the scale itself and its appropriate usage was likely demonstrated many times to LLMs during their unsupervised training phase.

To convert the ratings such as \texttt{\{\say{Supported}:2, \say{Refuted}:5, \say{Cherrypicking}:4, \say{NEE}:2\}} to a kind of probability distribution, we simply use softmax~\cite{NIPS1989_0336dcba}.
While the label probabilities are only emulated (and may only take a limited, discrete set of values) and the system may produce ties, it gets the job done until further research is carried out.

\subsubsection{Choosing LLM}
In our experiments, we have tested the full set of techniques introduced in this section, computing the text completion requests with:
\begin{enumerate}
    \item GPT-4o (version \texttt{2024-05-13})
    \item Claude-3.5-Sonnet (\texttt{2024-06-20}), using the Google's Vertex API
    \item LLaMA 3.1 70B, in the final experimets to see if the pipeline can be re-produced using open-source models
\end{enumerate} 

Their comparison can be seen in tables~\ref{tab:nli} and~\todo{tab}; for our submission in the \averitec{}  shared task, GPT-4o was used.