%!TEX ROOT=../emnlp2023.tex

\section{System description}
\label{sec:system}
Our system is designed to be simple and its core could be described as simply a RAG pipeline with customized retriever and LLM prompting (Listing~\ref{lst:llm_system_prompt}).
Despite that, this section describes and justifies our decisions taken at each step, our additions to the naive version of RAG modules to tune them for the specific task of fact-checking, and their impact on the system performance.

\subsection{Retrieval module}
\label{retrieval}
To ease comparison with the baseline and other systems designed for the task, our system does not use direct internet/search-engine access for its retrieval, but an \averitec{} \textit{knowledge store} provided alongside each claim.

To use our pipeline in the wild, our retrieval module is decoupled from the rest of the pipeline and can be swapped out in favour of an internet search module such as SerpApi\footnote{\url{https://serpapi.com/}} as a whole, or it can be used on top of a knowledge store emulated using search API or a cheap on-premise search over large crawled corpora such as CommonCrawl\footnote{\url{https://commoncrawl.org/}}.

\subsubsection{Knowledge stores}
Each claim's knowledge store contains pre-scraped results for various search queries that can be derived from the claim using human annotation or generative models.
The knowledge stores used with ours as well as the baseline system can be downloaded from the \averitec{}  dataset page\footnote{\url{https://fever.ai/dataset/averitec.html}}, containing about 1000 pre-scraped \textit{documents}\footnote{\label{devsetnote} The numbers are orientational and were computed on knowledge stores provided for the \averitec{}  dev set.}, each consisting of $28$ sentences at median\footnoteref{devsetnote}, albeit varying wildly between documents.
The methods used for generating the knowledge stores are explained in more detail by~\citet{averitec2024}.
%To use our system in the wild, this knowledge store can be emulated using a search API such as SerpApi, or even a large document collection such as Common Crawl pruned down to similar orders of magnitude using a cheap retrieval method and the claim as a search query.

Our retrieval module then focuses on picking a set of $k$ ($k=10$ in the examples below, as well as in our submitted system) most appropriate document chunks to fact-check the provided claim within this knowledge store.

\subsubsection{Angle-optimized embedding search}
\label{sec:knn}
Despite each article in any knowledge store only needing to be compared \textit{once} with its \textit{one specific} claim, which should be the use-case for CrossEncoder reranking~\cite{dejean2024thoroughcomparisoncrossencodersllms}, our empirical preliminary experiments made us favour a \textit{cosine-similarity} search based on vector embeddings instead.
It takes less time to embed the whole knowledge store into vectors than to match each document against a claim using crossencoder, and the produced embeddings can be re-used across experiments.

For our proof of concept, we explore the MTEB~\cite{muennighoff-etal-2023-mteb} benchmark leaderboard, looking for a reasonably-sized open-source embedding model, ultimately picking Mixedbread's mxbai-large-v1~\cite{li-li-2024-aoe,emb2024mxbai} optimized for the cosine objective fitting our inteded use.

To reduce querying time at a reasonable exactness tradeoff, we use Faiss~\cite{douze2024faiss,johnson2019billion} index to store our vectors, allowing us to only precompute semantical representation once, making the retriever respond rapidly in empirical experiments, allowing a very agile prototyping of novel methods to be used.

\subsubsection{Chunking with added context}
Our initial experiments with the whole \averitec{}  documents for the Document Retrieval step have revealed a significant weakness -- while most documents fit within the input size of the embedding model, outliers are common, often with \textit{hundreds of thousands} characters, exceeding the 512 input tokens with little to no coverage of their content.

Upon further examination, these are typically PDF documents of legislature, documentation and communication transcription -- highly relevant sources real fact-checker would scroll through to find the relevant part to refer. 

This workflow inspires the use of \textit{document chunk retrieval} as used in~\cite{rag}, commonly paired with RAG.
We partition each document into sets of its sentences of combined length of $N$ characters at most.
To take advantage of the full input size of the vector embedding model we use for semantical search, we set our bound $N=512*4=2048$, 512 being the input dimension of common embedding models, 4 often being used as a rule-of-thumb number of characters per token for US English in modern tokenizers~\cite{tokens}.

Importantly, each chunk is  assigned metadata -- the source URL, as well as the full text of the next and previous chunk within the same document.
This way, chunks can be presented to the LLM along with their original context in the generation module, where the length constraint is much less of an issue than in vector embedding.
As shown in~\cite{drchal2023pipelinedatasetgenerationautomated}, fact-checking models benefit from being exposed to larger pieces of text such as paragraphs or entire documents rather than out-of-context sentences.
Splitting our data into the maximum chunks that fit our retrieval model and providing them with additional context may help down the line, preventing the RAG sources from being semantically incomplete.

\subsubsection{Pruning the chunks}
While the chunking of long articles prevents their information from getting lost to retriever, it makes its search domain too large to embed on demand.
As each of the thousands of claims has its own knowledge store, each of possibly tens of thousands of chunks, we seek to omit the chunks having little to no common tokens with our claim using an efficient BM25~\cite{bm25} search for the nearest $\omega$ chunks, setting the $\omega$ to 6000 for dev and 2000 for test claims. 
This yields a reasonably-sized document store for embedding each chunk into a vector, taking an average of 40 s to compute and store using the method described in Section~\ref{sec:knn} for each dev-claim using our Tesla V100 GPU.

This allows a quick and agile production of vectorstores for further querying and experimentation, motivated by the \averitec{}  test data being published just several days before the announced submission deadline.
The pruning also keeps the resource intensity moderate for real-world applications -- if time is not of the essence, the step can be omitted.

\subsubsection{Diversifying sources: MMR}
While the embedding search based of the entire claim rather than generating its \say{search queries} introduces less noise and captures the semantics of the whole claim, it is, however, prone to redundancy among search results, which we address using a reranking by the results' Maximal Marginal Relevance (MMR)~\cite{carbonell-mmr}, a metric popular for the RAG task, which maximizes the search results' score computed as (for $D_i\in P$)
$$\lambda \cdot \mathrm{Sim}(D_i, Q) - (1-\lambda) \cdot \max_{D_j \in S} \mathrm{Sim}(D_i, D_j)$$
$Sim$ denoting the cosine-similarity between embeddings, $Q$ being the search query, and $P$ the pre-fetched set of documents (by a search which simply maximizes their $Sim$ to $Q$), forming $S$ as the final search result, by adding each $D_i$ as MMR-argmax one by one, until reaching its desired size.

In our system, we set $\lambda=0.75$ to favour relevancy rather than diversity, $|S|=10$ and $|P| = 40$, obtaining a set of diverse sources relevant to each claim at a fraction of cost and complexity of a query-generation driven retrieval, such as that used in~\cite{averitec2024}.

\subsection{Evidence \& label generator}
\label{sec:generation}
The second and the last module on our proposed pipeline for automated fact checking is the Evidence \& Label Generator, which receives a claim and $k$ sources (document chunks), and returns $l$ (in our case, $l=10$) question-answer pairs of evidence abstracted from the sources, along with the veracity verdict -- in \averitec{} dataset, a claim may be classified as \textit{Supported}, \textit{Refuted}, \textit{Not Enough Evidence}, or \textit{Conflicting Evidence/Cherrypicking} with respect to its evidence.

Our approach leverages a Large Language Model (LLM), instructing it to output both evidence and the label in a single step, as a chain of thought.
We rely on JSON-structured output generation with source referencing using a numeric identifier, we estimate the label confidences using Likert-scale ratings.
The full system prompt can be examined in Listing~\ref{lst:llm_system_prompt} and this section further explains the choices behind it.

\subsubsection{JSON generation}

To be able to collect LLM's results programmatically, we exploit their capability to produce structured outputs, which is on rise, with datasets available for tuning~\cite{tang2024strucbenchlargelanguagemodels} and by the time of writing of this paper (August 2024), systems for strictly structured prediction are beginning to be launched by major providers~\cite{json}.

Despite not having access to such structured-prediction API by the time of \averitec{} shared task, the current generation of models examined for the task (section~\ref{sec:chosen_llms}) rarely strays from the desired format if properly explained within a system prompt -- we instruct our models to output a JSON of pre-defined properties (see prompt Listing~\ref{lst:llm_system_prompt}) featuring both evidence and the veracity verdict for a given claims.

Although we implement fallbacks, less than 0.5\% of our predictions result in a parsing exception throughout experimentation, and could be easily recovered using the same prompting again, exploiting the intrinsic randomness of LLM predictions.

\subsubsection{Chain-of-thought prompting}
While JSON dictionary should be order-invariant, we can actually exploit the order of outputs in our output structure to make LLMS like GPT-4o output better results.~\cite{cot}
This is commonly referred to as the \say{chain-of-thought} prompting -- if we instruct the autoregressive LLM to first output the evidence (question, then answer), then a set of all labels with their confidence ratings (see section~\ref{likert}) and only then the final verdict, its prediction is both cheaper as opposed to implementing an extra module, as well as more reliable, as it must attend to all of the intermediate steps as well.

\subsubsection{Source referring}
To be able to backtrack the generated evidence to the urls of the used sources, we simply augment each question-answer pair with a source field.
We assign a 1-based index to each of the sources to facilitate tokenization and prompt the LLM to refer it as the source ID with each evidence it generates.
While hallucination can not be fully prevented, it is less common than it may appear -- with RAG gaining popularity, the models are being trained to cite their sources using special citation tokens~\cite{menick2022teachinglanguagemodelssupport}, not dissimilarly to our proposal.

\subsubsection{Dynamic few-shot learning}
To utilise the few-shot learning framework~\cite{fewshot} shown to increase quality of model output, we provide our LLMs with examples of what we expect the model to do.
To obtain such examples, our evidence generator looks up the \averitec{} train set using BM25 to get the 10 most similar claims, providing them as the few-shot examples, along their gold evidence and veracity verdicts.
Experimentally, we also few-shot our models to output an \textit{answer type} (\textit{Extractive}, \textit{Abstractive}, \textit{Boolean},\dots) as the \textit{answer type} is listed with each sample anyways, and we have observed its integration into the generation task to slightly boost our model performance.

\subsubsection{Likert-scale label confidences}
\label{likert}
Despite modern LLMs being well capable of predicting the label in a \say{pick one} fashion, research applications such as ours may prefer them to output a probability distribution over all labels for two reasons.

Firstly, it measures the confidence in each label, pinpointing the edge-cases, secondly, it allows ensembling the LLM classification with any other model, such as Encoders with classification head finetuned on the task of Natural Language Inference (NLI) (see section~\ref{subsubsec:ensembling}).

As the LLMs and other token prediction schemes struggle with the prediction of continuous numbers which are notoriously hard to tokenize appropriately~\cite{golkar2023xvalcontinuousnumberencoding}, we come up with a simple alternative: instructing the model to print each of the 4 possible labels, along with their Likert-scale rating: 1 for \say{strongly disagree}, 2 for \say{disagree}, 3 for \say{neutral}, 4 for \say{agree} and 5 for \say{strongly agree}~\cite{likert1932technique}.

On top of the ease of tokenization, Likert scale's popularity in psychology and other fields such as software testing~\cite{likertstudy} adds another benefit -- both the scale itself and its appropriate usage were likely demonstrated many times to LLMs during their unsupervised training phase.

To convert the ratings such as \texttt{\{\say{Supported}:2, \say{Refuted}:5, \say{Cherrypicking}:4, \say{NEE}:2\}} to a probability distribution, we simply use softmax~\cite{NIPS1989_0336dcba}.
While the label probabilities are only emulated (and may only take a limited, discrete set of values) and the system may produce ties, it gets the job done until further research is carried out.

\subsubsection{Choosing LLM}
\label{sec:chosen_llms}
In our experiments, we have tested the full set of techniques introduced in this section, computing the text completion requests with:
\begin{enumerate}
    \item GPT-4o (version \texttt{2024-05-13})
    \item Claude-3.5-Sonnet (\texttt{2024-06-20}), using the Google's Vertex API
    \item LLaMA 3.1 70B, in the final experimets to see if the pipeline can be re-produced using open-source models
\end{enumerate} 

Their comparison can be seen in tables~\ref{tab:nli} and~\ref{tab:pipeline_scores}; for our submission in the \averitec{}  shared task, GPT-4o was used.