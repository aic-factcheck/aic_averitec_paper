%!TEX ROOT=../emnlp2023.tex

\section{System Description}
\label{sec:system}
\todo{write}
Our syste

\subsection{Retrieval Module}
To ease comparison with the baseline and other systems designed for the task, our system does not use direct internet/search-engine access for its retrieval, but a \textit{knowledge store} provided separately alongside each claim.

\subsubsection{Knowledge Stores}
Each claim's knowledge store contains pre-scraped search results for various queries that can be derived from the claim using human annotation or generative models.
The knowledge stores used with ours as well as the baseline system can be downloaded from the Averitec dataset page\footnote{\url{https://fever.ai/dataset/averitec.html}}, containing about 1000 pre-scraped \textit{documents}\footnote{\label{devsetnote} The numbers are orientational and were established using the dev knowledge store}, each consisting of $28$ sentences at median\footnoteref{devsetnote}, albeit varying wildly between documents.

To use our system in the wild, this knowledge store can be emulated using a search API such as SerpApi, or even a large document collection such as Common Crawl pruned down to similar orders of magnitude using a cheap retrieval method and the claim as a search query.

Our retrieval module then focuses on picking a set of $k$ ($k=10$ in the examples below) most appropriate document chunks to fact-check the provided claim within this knowledge store.

\subsubsection{Chunking}
Our initial experiments with the whole AVerITeC documents for the Document Retrieval step have revealed a significant weakness -- while the median document length (about 2000 characters) fits the input size of our embedding model with a generous margin, there is often a small number of documents with \textit{hundreds of thousands} characters, exceeding the 1024 input tokens with little to no coverage.

Upon further examination, these have more often than not, been PDF documents of legislature, documentation and transcription -- highly relevant sources real fact-checker would scroll through to find the relevant part to refer. 

This workflow has inspired our next approach -- to facilitate the retrieval of smaller articles as a whole and 

\subsubsection{Search-space Pruning}
While the chunking of long articles prevents information from larger documents from being lost, it makes the domain for embedding search too large.
As every claim has its own retrieval domain of tens of thousands of chunks, we seek to omit the chunks having little to no common tokens with our claim using a BM25 search for the nearest $k$ chunks, setting the $k$ to 6000 for dev and 2000 for test claims. 
This yields a reasonably-sized document store for embedding into a vectorstore, taking about 40s to compute and store within FAISS~\cite{douze2024faiss} for each test-claim using our Tesla V100 GPU.

This allows a quick and agile production of vectorstores for further querying and experimentation, motivated by the Averitec test data only being published just several days before the announced submission deadline while also keeping the resource intensity moderate for real-world applications -- if time is not of the essence, the step can be omitted.

\subsubsection{Angle-optimized embedding search}
Mixedbread~\cite{li-li-2024-aoe,emb2024mxbai}, Faiss~\cite{douze2024faiss,johnson2019billion}

\subsubsection{Looking for diversity}
While the original~\cite{averitec2024} baseline retrieved articles based on different queries to promote variety among search results, our approach omits the division of a claim to a set of different queries.
We aim to use an embedding-driven similarity search in the neighbourhood of the whole original claim not to leave any piece of information behind or introduce noise in yet another generative task along the pipeline.

Our solution is, however, prone to redundancy among search results, which we address using a reranking by the results' Maximal Marginal Relevance (MMR)~\cite{carbonell-mmr}, a metric popular for the RAG task computed as
$$\lambda \cdot \mathrm{Sim}(D_i, Q) - (1-\lambda) \cdot \max_{D_j \in S} \mathrm{Sim}(D_i, D_j)$$

In our system, we set $\lambda=0.75$ to favour relevancy rather than diversity, $k=10$ and $k_{fetch} = 40$, obtaining a set of diverse sources relevant to each claim at a fraction of cost and complexity of a query-generation driven retrieval, such as that used in~\cite{averitec2024}.

\subsection{Evidence Generation}
Rather than sampling evidence from retrieved text or QG+QA pipeline ran on the retrieved sentences, we argue that the modern LLMs offer the use of wider context, such as whole news articles.

\subsubsection{\texttt{JSON} Generation}
The current LLMs are trained very well for this, allows for very simple integration of LLM into pipeline

\subsubsection{Source Referring}
We assign a 1-based index to each of the sourced chunks and prompt the LLM to refer it as the source ID with each evidence it generates.
This has been shown quite reliable in \todo{literature}.

\subsubsection{Chain-of-thoughts Prompting}
While JSON dictionary should be order-invariant, we can actually exploit the order of outputs to make LLMS like GPT-4o output better results.

\subsubsection{Few-shot learning}
To further boost our system's evidence-generation capabilities, 


